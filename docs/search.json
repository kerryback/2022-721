[
  {
    "objectID": "01.html#section",
    "href": "01.html#section",
    "title": "Trees and Nets",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "01.html#section-1",
    "href": "01.html#section-1",
    "title": "Trees and Nets",
    "section": "",
    "text": "y = 2*3"
  },
  {
    "objectID": "01.html#decision-tree",
    "href": "01.html#decision-tree",
    "title": "Trees and Nets",
    "section": "Decision tree",
    "text": "Decision tree\n\nSplit sample sucessively into smaller subsamples by answering “yes-no” questions.\nEach question is based on a single variable: is it above a threshold?\nSplit a specified number of times (depth). Final subsamples are called leaves.\nFor regression problems, the prediction for each observation is the mean of the training observations that end up in the same leaf.\nFor classification problems the prediction is the category that occurs most frequently in the leaf."
  },
  {
    "objectID": "01.html#example-of-a-tree",
    "href": "01.html#example-of-a-tree",
    "title": "Trees and Nets",
    "section": "Example of a tree",
    "text": "Example of a tree"
  },
  {
    "objectID": "01.html#decision-tree-splitting-for-regression",
    "href": "01.html#decision-tree-splitting-for-regression",
    "title": "Trees and Nets",
    "section": "Decision tree splitting for regression",
    "text": "Decision tree splitting for regression\n\nAt each step, the algorithm chooses a variable and threshold to split on.\nIn a regression problem, the standard decision criterion is SSE.\n\nError = actual - mean of subset\nOutliers tend to get separated into their own subsets (less so for SAE)\n\nThere are also other relatively standard criteria, for example in scikit-learn."
  },
  {
    "objectID": "01.html#decision-tree-splitting-for-classification",
    "href": "01.html#decision-tree-splitting-for-classification",
    "title": "Trees and Nets",
    "section": "Decision tree splitting for classification",
    "text": "Decision tree splitting for classification\n\nLet \\(k\\) = number of classes. Suppose a split results in \\(n\\) observations in a subset. Let \\(x_i=\\) fraction in class \\(i\\).\nDefault criterion for scikit-learn is Gini = \\(1 - \\sum_{i=1}^k x_i^2\\)\nIf we consider \\(\\sum x_i^2\\) subject to \\(\\sum x_i=n\\), the value is maximized at \\(x_i=1/n\\) and minimized at boundaries: some \\(x_i=1\\) and others equal 0.\n\nSo, maximizing Gini means trying to create “pure” subsets (all of same type).\nThere are also other standard criteria, for example entropy and log loss."
  },
  {
    "objectID": "01.html#random-forest",
    "href": "01.html#random-forest",
    "title": "Trees and Nets",
    "section": "Random forest",
    "text": "Random forest\n\nMultiple trees fit to random data\nData for each tree is a bootstrapped sample:\n\nrandom selection of rows (with replacement)\nsame size as original sample\n\nPrediction is average of predictions of the trees\nHyperparameters = number of trees and depth of trees"
  },
  {
    "objectID": "01.html#boosting",
    "href": "01.html#boosting",
    "title": "Trees and Nets",
    "section": "Boosting",
    "text": "Boosting\n\nBoosting means combining models, for example trees.\nPrediction is sum of individual predictions.\nGradient boosting fits second model to errors of first, third to errors of first two combined, …\nAdaptive boosting fits original target but overweights errors from prior model\nLearning rate (weight to put on new model) is a hyperparameter"
  },
  {
    "objectID": "01.html#multi-layer-perceptrons",
    "href": "01.html#multi-layer-perceptrons",
    "title": "Trees and Nets",
    "section": "Multi-layer perceptrons",
    "text": "Multi-layer perceptrons\n\nA multi-layer perceptron (MLP) consists of “neurons” arranged in layers.\nA neuron is a mathematical function. It takes inputs \\(x_1, \\ldots, x_n\\), calculates a function \\(y=f(x_1, \\ldots, x_n)\\) and passes \\(y\\) to the neurons in the next level.\nThe inputs in the first layer are the predictors.\nThe inputs in successive layers are the calculations from the prior level.\nThe last layer is a single neuron that produces the output."
  },
  {
    "objectID": "01.html#illustration",
    "href": "01.html#illustration",
    "title": "Trees and Nets",
    "section": "Illustration",
    "text": "Illustration\n\n\n\n\n\n\n\ninput is \\(x \\in \\mathbb{R}^4\\)\nfunctions \\(f_1, \\ldots, f_5\\) of \\(x\\) are calculated (called “hidden layer”)\noutput is \\(g(f_1(x), \\ldots, f_5(x))\\)"
  },
  {
    "objectID": "01.html#rectified-linear-units",
    "href": "01.html#rectified-linear-units",
    "title": "Trees and Nets",
    "section": "Rectified linear units",
    "text": "Rectified linear units\n\nThe usual function for the neurons (except in the last layer) is \\[ y = \\max(0,b+w_1x_1 + \\cdots + w_nx_n)\\] Parameters \\(b\\) (called bias) and \\(w_1, \\ldots w_n\\) (called weights) are different for different neurons.\nThis function is called a rectified linear unit (RLU).\n\nThere are other choices. This function in general is called the “activation function.”"
  },
  {
    "objectID": "01.html#analogy-to-neurons-firing",
    "href": "01.html#analogy-to-neurons-firing",
    "title": "Trees and Nets",
    "section": "Analogy to neurons firing",
    "text": "Analogy to neurons firing\nIf \\(w_i>0\\) and \\(b<0\\), then \\(y>0\\) only when \\(x_i\\) are large enough.\nA neuron fires when it is sufficiently stimulated by signals from other neurons (in prior layer)."
  },
  {
    "objectID": "01.html#neural-net-output-function",
    "href": "01.html#neural-net-output-function",
    "title": "Trees and Nets",
    "section": "Neural net output function",
    "text": "Neural net output function\nFor regression problems, the output function is linear \\[ y = b+w_1x_1 + \\cdots + w_nx_n\\].\n\nFor classification, there is a linear function for each class and the output is the set of probabilities \\(e^{y_i}/\\sum e^{y_j}\\)."
  },
  {
    "objectID": "01.html#deep-learning",
    "href": "01.html#deep-learning",
    "title": "Trees and Nets",
    "section": "Deep learning",
    "text": "Deep learning\n\nDeep learning means a neural network with many layers. It is behind facial recognition, self-driving cars, …\nNeed specialized library, probably TensorFlow (from Google) or PyTorch (from Facebook), and probably need graphical processing units (GPUs) – i.e., run on video cards\nHands-On Machine Learning with Scikit-Learn and TensorFlow\nDeep Learning for Coders with fastai and PyTorch (also fastai website)"
  }
]